{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ec6e71e-3988-4ddd-a02d-cd50fdda08e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "\n",
    "import re\n",
    "import jieba\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63661c2-840d-4574-ba76-4010e21a70b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不用自己下载，直接使用已下载好的数据\n",
    "def get_comments(url):\n",
    "    comments = []\n",
    "    resp = requests.get(url)\n",
    "    resp.encoding = 'gbk'\n",
    "\n",
    "    if resp.status_code != 200:\n",
    "        return []\n",
    "\n",
    "    content = resp.text\n",
    "    if content:\n",
    "        ind = content.find('(')\n",
    "        s1 = content[ind + 1 : -2]\n",
    "        try:\n",
    "            js = json.loads(s1)\n",
    "            comment_infos = js['comments']\n",
    "        except:\n",
    "            print('error')\n",
    "            return([])\n",
    "\n",
    "        for comment_info in comment_infos:\n",
    "            comment_content = comment_info['content']\n",
    "            str1 = comment_content + '\\n'\n",
    "            comments.append(str1)\n",
    "    return comments\n",
    "\n",
    "good_comments = []\n",
    "good_comment_url_templates = [\n",
    "    '',\n",
    "    '',\n",
    "    '',\n",
    "    '',\n",
    "    ''\n",
    "]\n",
    "\n",
    "j = 0\n",
    "for good_comment_url_template in good_comment_url_templates:\n",
    "    # 模拟翻页100次\n",
    "    for i in range(100):\n",
    "        url = good_comment_url_template.format(i)\n",
    "        good_comments += get_comments(url)\n",
    "        print('第 {} 条记录，总文本长度: {}'.format(j, len(good_comments)))\n",
    "        j += 1\n",
    "\n",
    "fw = open('data/good.txt', 'w', encoding = 'utf-8')\n",
    "fw.writelines(good_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d592d34-9fd2-4d1c-9065-f3892e63690d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emotion_classifier_data/good.txt include 8089 lines, 100839 words.\n",
      "emotion_classifier_data/bad.txt include 5076 lines, 56070 words.\n",
      "diction size: 7135\n"
     ]
    }
   ],
   "source": [
    "good_file = 'emotion_classifier_data/good.txt'\n",
    "bad_file = 'emotion_classifier_data/bad.txt'\n",
    "\n",
    "def filter_punc(sentence):\n",
    "    sentence = re.sub(\"[\\\\s+\\\\.\\\\!\\\\/_,$%^*(+\\\\\\\"\\\\'“”《》?“]+|[+——！，。？、~@#￥%……&*（）：]+\", \"\", sentence)\n",
    "    return(sentence)\n",
    "\n",
    "def prepare_data(good_file, bad_file, is_filter=True):\n",
    "    all_words = []\n",
    "    pos_sentences = []\n",
    "    neg_sentences = []\n",
    "    with open(good_file, 'r', encoding='utf-8') as fr:\n",
    "        for idx, line in enumerate(fr):\n",
    "            if is_filter:\n",
    "                line = filter_punc(line)\n",
    "            words = jieba.lcut(line) # 分词\n",
    "            if len(words) > 0:\n",
    "                all_words += words\n",
    "                pos_sentences.append(words)\n",
    "    print('{0} include {1} lines, {2} words.'.format(good_file, idx +  1, len(all_words)))\n",
    "\n",
    "    count = len(all_words)\n",
    "    with open(bad_file, 'r', encoding='utf-8') as fr:\n",
    "        for idx, line in enumerate(fr):\n",
    "            if is_filter:\n",
    "                line = filter_punc(line)\n",
    "            words = jieba.lcut(line) # 分词\n",
    "            if len(words) > 0:\n",
    "                all_words += words\n",
    "                neg_sentences.append(words)\n",
    "    print('{0} include {1} lines, {2} words.'.format(bad_file, idx +  1, len(all_words) - count))\n",
    "\n",
    "    # 建立词典: word -> [id, 频率]\n",
    "    diction = {}\n",
    "    cnt = Counter(all_words) # 统计频率\n",
    "    for word, freq in cnt.items():\n",
    "        diction[word] = [len(diction), freq]\n",
    "    print('diction size: {}'.format(len(diction)))\n",
    "    return (pos_sentences, neg_sentences, diction)\n",
    "            \n",
    "pos_sentences, neg_sentences, diction = prepare_data(good_file, bad_file, True)\n",
    "st = sorted([(v[1], w) for w, v in diction.items()])\n",
    "\n",
    "def word2index(word, diction):\n",
    "    if word in diction:\n",
    "        value = diction[word][0]\n",
    "    else:\n",
    "        value = -1\n",
    "    return value\n",
    "\n",
    "def index2word(index, diction):\n",
    "    for w, v in diction.items():\n",
    "        if v[0] == index:\n",
    "            return w\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0aaa9e86-1a34-4b82-b6d9-60b8102a254e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "不错\n"
     ]
    }
   ],
   "source": [
    "print(word2index('不错', diction))\n",
    "print(index2word(13, diction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "605e8e31-ef33-436b-8138-36dba135cfbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "7135\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.33333333 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "['布料', '很', '好']\n"
     ]
    }
   ],
   "source": [
    "# 句子的向量化\n",
    "# 向量的尺寸是词典中词汇的个数，i位置上的数值为第i个单词出现的频率\n",
    "def sentence2vec(sentence, dictionary):\n",
    "    vector = np.zeros(len(dictionary))\n",
    "    for l in sentence:\n",
    "        vector[l] += 1\n",
    "    return 1.0 * vector / len(sentence)  # 压缩到0~1之间\n",
    "\n",
    "dataset= []\n",
    "labels = []\n",
    "sentences = [] # 原始句子，调试用 \n",
    "\n",
    "#  处理正向评论\n",
    "for sentence in pos_sentences:\n",
    "    new_sentence_index = []\n",
    "    for l in sentence: # 已经分词了\n",
    "        if l in diction:\n",
    "            new_sentence_index.append(word2index(l, diction))\n",
    "    dataset.append(sentence2vec(new_sentence_index, diction))\n",
    "    labels.append(0) # 0代表正标签\n",
    "    sentences.append(sentence)\n",
    "\n",
    "#  处理负向评论\n",
    "for sentence in neg_sentences:\n",
    "    new_sentence_index = [] \n",
    "    for l in sentence: # 已经分词了\n",
    "        if l in diction:\n",
    "            new_sentence_index.append(word2index(l, diction))\n",
    "    dataset.append(sentence2vec(new_sentence_index, diction))\n",
    "    labels.append(1) # 1代表负标签\n",
    "    sentences.append(sentence)\n",
    "\n",
    "indices = np.random.permutation(len(dataset))\n",
    "dataset = [dataset[i] for i in indices]\n",
    "labels = [labels[i] for i in indices]\n",
    "sentences = [sentences[i] for i in indices]\n",
    "print(len(dataset[0]))\n",
    "print(dataset[0][30:128])\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "771ba6aa-867d-455a-a639-84eed4fa318b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1303\n"
     ]
    }
   ],
   "source": [
    "test_size = len(dataset) // 10\n",
    "train_data = dataset[2 * test_size :]\n",
    "train_label = labels[2 * test_size :]\n",
    "\n",
    "validate_data = dataset[: test_size]\n",
    "validate_label = labels[: test_size]\n",
    "\n",
    "test_data = dataset[test_size: 2 * test_size]\n",
    "test_label = labels[test_size: 2 * test_size]\n",
    "\n",
    "print(len(validate_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453ad693-412c-46a6-90ca-30f44b185bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 0 轮，训练损失: 0.89，校验损失: 0.75，校验准确率: 0.39\n",
      "第 0 轮，训练损失: 0.66，校验损失: 0.64，校验准确率: 0.61\n",
      "第 0 轮，训练损失: 0.64，校验损失: 0.56，校验准确率: 0.69\n",
      "第 0 轮，训练损失: 0.60，校验损失: 0.48，校验准确率: 0.73\n",
      "第 1 轮，训练损失: 0.58，校验损失: 0.43，校验准确率: 0.87\n",
      "第 1 轮，训练损失: 0.55，校验损失: 0.40，校验准确率: 0.87\n",
      "第 1 轮，训练损失: 0.53，校验损失: 0.38，校验准确率: 0.88\n",
      "第 1 轮，训练损失: 0.51，校验损失: 0.37，校验准确率: 0.87\n",
      "第 2 轮，训练损失: 0.50，校验损失: 0.36，校验准确率: 0.89\n",
      "第 2 轮，训练损失: 0.49，校验损失: 0.35，校验准确率: 0.89\n",
      "第 2 轮，训练损失: 0.48，校验损失: 0.34，校验准确率: 0.89\n",
      "第 2 轮，训练损失: 0.47，校验损失: 0.35，校验准确率: 0.88\n",
      "第 3 轮，训练损失: 0.47，校验损失: 0.33，校验准确率: 0.90\n",
      "第 3 轮，训练损失: 0.46，校验损失: 0.33，校验准确率: 0.90\n",
      "第 3 轮，训练损失: 0.45，校验损失: 0.32，校验准确率: 0.90\n",
      "第 3 轮，训练损失: 0.44，校验损失: 0.33，校验准确率: 0.88\n",
      "第 4 轮，训练损失: 0.44，校验损失: 0.32，校验准确率: 0.90\n",
      "第 4 轮，训练损失: 0.43，校验损失: 0.31，校验准确率: 0.90\n",
      "第 4 轮，训练损失: 0.43，校验损失: 0.31，校验准确率: 0.90\n",
      "第 4 轮，训练损失: 0.42，校验损失: 0.32，校验准确率: 0.89\n",
      "第 5 轮，训练损失: 0.42，校验损失: 0.31，校验准确率: 0.90\n",
      "第 5 轮，训练损失: 0.42，校验损失: 0.30，校验准确率: 0.90\n",
      "第 5 轮，训练损失: 0.41，校验损失: 0.30，校验准确率: 0.90\n",
      "第 5 轮，训练损失: 0.41，校验损失: 0.31，校验准确率: 0.89\n",
      "第 6 轮，训练损失: 0.41，校验损失: 0.30，校验准确率: 0.90\n",
      "第 6 轮，训练损失: 0.40，校验损失: 0.29，校验准确率: 0.91\n",
      "第 6 轮，训练损失: 0.40，校验损失: 0.29，校验准确率: 0.91\n",
      "第 6 轮，训练损失: 0.40，校验损失: 0.31，校验准确率: 0.89\n"
     ]
    }
   ],
   "source": [
    "# 构建神经网络\n",
    "m = nn.ReLU()\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(len(diction), 10),\n",
    "    nn.ReLU(), # max(0,x)\n",
    "    nn.Linear(10, 2),\n",
    "    nn.LogSoftmax(dim=1), # dim=1按照第二个维度计算输出，也就是输出每一个样本在各个类别的概率\n",
    ")\n",
    "\n",
    "# 计算分类准确度\n",
    "def rightness(predictions, labels):\n",
    "    # max函数返回[value, index], pred得到最大概率的索引，0表示好，1表示负面\n",
    "    pred = torch.max(predictions.data, 1)[1] # max的dim=1 表示沿着第二个维度计算max，即好或坏两个分类数值， \n",
    "\n",
    "    rights = pred.eq(labels.data.view_as(pred)).sum()\n",
    "\n",
    "    return rights, len(labels)\n",
    "\n",
    "# 对于分类问题 损失函数为交叉熵\n",
    "cost = nn.NLLLoss()\n",
    "# 优化器，自动调节学习率\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "records = []\n",
    "\n",
    "losses = []\n",
    "for epoch in range(10):\n",
    "    for i, data in enumerate(zip(train_data, train_label)):\n",
    "        x, y = data\n",
    "        x = torch.FloatTensor(x).view(1, -1) #[1, len(diction)]\n",
    "        y = torch.LongTensor(np.array([y])) #[1,1]\n",
    "\n",
    "        optimizer.zero_grad(); # 清空梯度\n",
    "        predict = model(x) \n",
    "        loss = cost(predict, y)\n",
    "\n",
    "        losses.append(loss.data.numpy())\n",
    "\n",
    "        loss.backward() # 梯度反向传播\n",
    "        optimizer.step() # 开始对参数进一步优化\n",
    "\n",
    "        # 运行校验集的数据，只是验证结果，不能反向传播梯度参与训练\n",
    "        if i % 3000 == 0:\n",
    "            val_losses = []\n",
    "            rights = []\n",
    "            for j, val in enumerate(zip(validate_data, validate_label)):\n",
    "                x, y = val\n",
    "                x = torch.FloatTensor(x).view(1, -1) #[1, len(diction)]\n",
    "                y = torch.LongTensor(np.array([y])) #[1,1]\n",
    "\n",
    "                predict = model(x)\n",
    "                right = rightness(predict, y) # 计算准确度\n",
    "                rights.append(right)\n",
    "                \n",
    "                loss = cost(predict, y)\n",
    "                val_losses.append(loss.data.numpy())\n",
    "            # 计算校验集的平均准确度\n",
    "            right_ratio = 1.0 * np.sum(np.fromiter((i[0] for i in rights), int)) / np.sum(np.fromiter((i[1] for i in rights), int))\n",
    "\n",
    "            print('第 {} 轮，训练损失: {:.2f}，校验损失: {:.2f}，校验准确率: {:.2f}'\n",
    "                  .format(epoch, np.mean(losses), np.mean(val_losses), right_ratio))\n",
    "            records.append([np.mean(losses), np.mean(val_losses), right_ratio])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38258f0b-35ff-4074-8049-66a849ddcc41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
