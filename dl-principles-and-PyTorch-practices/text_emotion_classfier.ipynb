{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ec6e71e-3988-4ddd-a02d-cd50fdda08e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "\n",
    "import re\n",
    "import jieba\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63661c2-840d-4574-ba76-4010e21a70b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不用自己下载，直接使用已下载好的数据\n",
    "def get_comments(url):\n",
    "    comments = []\n",
    "    resp = requests.get(url)\n",
    "    resp.encoding = 'gbk'\n",
    "\n",
    "    if resp.status_code != 200:\n",
    "        return []\n",
    "\n",
    "    content = resp.text\n",
    "    if content:\n",
    "        ind = content.find('(')\n",
    "        s1 = content[ind + 1 : -2]\n",
    "        try:\n",
    "            js = json.loads(s1)\n",
    "            comment_infos = js['comments']\n",
    "        except:\n",
    "            print('error')\n",
    "            return([])\n",
    "\n",
    "        for comment_info in comment_infos:\n",
    "            comment_content = comment_info['content']\n",
    "            str1 = comment_content + '\\n'\n",
    "            comments.append(str1)\n",
    "    return comments\n",
    "\n",
    "good_comments = []\n",
    "good_comment_url_templates = [\n",
    "    '',\n",
    "    '',\n",
    "    '',\n",
    "    '',\n",
    "    ''\n",
    "]\n",
    "\n",
    "j = 0\n",
    "for good_comment_url_template in good_comment_url_templates:\n",
    "    # 模拟翻页100次\n",
    "    for i in range(100):\n",
    "        url = good_comment_url_template.format(i)\n",
    "        good_comments += get_comments(url)\n",
    "        print('第 {} 条记录，总文本长度: {}'.format(j, len(good_comments)))\n",
    "        j += 1\n",
    "\n",
    "fw = open('data/good.txt', 'w', encoding = 'utf-8')\n",
    "fw.writelines(good_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d592d34-9fd2-4d1c-9065-f3892e63690d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emotion_classifier_data/good.txt include 8089 lines, 100839 words.\n",
      "emotion_classifier_data/bad.txt include 5076 lines, 56070 words.\n",
      "diction size: 7135\n"
     ]
    }
   ],
   "source": [
    "good_file = 'emotion_classifier_data/good.txt'\n",
    "bad_file = 'emotion_classifier_data/bad.txt'\n",
    "\n",
    "def filter_punc(sentence):\n",
    "    sentence = re.sub(\"[\\\\s+\\\\.\\\\!\\\\/_,$%^*(+\\\\\\\"\\\\'“”《》?“]+|[+——！，。？、~@#￥%……&*（）：]+\", \"\", sentence)\n",
    "    return(sentence)\n",
    "\n",
    "def prepare_data(good_file, bad_file, is_filter=True):\n",
    "    all_words = []\n",
    "    pos_sentences = []\n",
    "    neg_sentences = []\n",
    "    with open(good_file, 'r', encoding='utf-8') as fr:\n",
    "        for idx, line in enumerate(fr):\n",
    "            if is_filter:\n",
    "                line = filter_punc(line)\n",
    "            words = jieba.lcut(line) # 分词\n",
    "            if len(words) > 0:\n",
    "                all_words += words\n",
    "                pos_sentences.append(words)\n",
    "    print('{0} include {1} lines, {2} words.'.format(good_file, idx +  1, len(all_words)))\n",
    "\n",
    "    count = len(all_words)\n",
    "    with open(bad_file, 'r', encoding='utf-8') as fr:\n",
    "        for idx, line in enumerate(fr):\n",
    "            if is_filter:\n",
    "                line = filter_punc(line)\n",
    "            words = jieba.lcut(line) # 分词\n",
    "            if len(words) > 0:\n",
    "                all_words += words\n",
    "                neg_sentences.append(words)\n",
    "    print('{0} include {1} lines, {2} words.'.format(bad_file, idx +  1, len(all_words) - count))\n",
    "\n",
    "    # 建立词典: word -> [id, 频率]\n",
    "    diction = {}\n",
    "    cnt = Counter(all_words) # 统计频率\n",
    "    for word, freq in cnt.items():\n",
    "        diction[word] = [len(diction), freq]\n",
    "    print('diction size: {}'.format(len(diction)))\n",
    "    return (pos_sentences, neg_sentences, diction)\n",
    "            \n",
    "pos_sentences, neg_sentences, diction = prepare_data(good_file, bad_file, True)\n",
    "st = sorted([(v[1], w) for w, v in diction.items()])\n",
    "\n",
    "def word2index(word, diction):\n",
    "    if word in diction:\n",
    "        value = diction[word][0]\n",
    "    else:\n",
    "        value = -1\n",
    "    return value\n",
    "\n",
    "def index2word(index, diction):\n",
    "    for w, v in diction.items():\n",
    "        if v[0] == index:\n",
    "            return w\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0aaa9e86-1a34-4b82-b6d9-60b8102a254e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "不错\n"
     ]
    }
   ],
   "source": [
    "print(word2index('不错', diction))\n",
    "print(index2word(13, diction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "605e8e31-ef33-436b-8138-36dba135cfbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.125 0.125 0.125 0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.125 0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.   ]\n",
      "['老公', '很', '喜欢', '面料', '不错', '大小', '也', '合适']\n"
     ]
    }
   ],
   "source": [
    "# 句子的向量化\n",
    "# 向量的尺寸是词典中词汇的个数，i位置上的数值为第i个单词出现的频率\n",
    "def sentence2vec(sentence, dictionary):\n",
    "    vector = np.zeros(len(dictionary))\n",
    "    for l in sentence:\n",
    "        vector[l] += 1\n",
    "    return 1.0 * vector / len(sentence)  # 压缩到0~1之间\n",
    "\n",
    "dataset= []\n",
    "labels = []\n",
    "sentences = [] # 原始句子，调试用 \n",
    "\n",
    "#  处理正向评论\n",
    "for sentence in pos_sentences:\n",
    "    new_sentence_index = []\n",
    "    for l in sentence: # 已经分词了\n",
    "        if l in diction:\n",
    "            new_sentence_index.append(word2index(l, diction))\n",
    "    dataset.append(sentence2vec(new_sentence_index, diction))\n",
    "    labels.append(0) # 0代表正标签\n",
    "    sentences.append(sentence)\n",
    "\n",
    "#  处理负向评论\n",
    "for sentence in neg_sentences:\n",
    "    new_sentence_index = []\n",
    "    for l in sentence: # 已经分词了\n",
    "        if l in diction:\n",
    "            new_sentence_index.append(word2index(l, diction))\n",
    "    dataset.append(sentence2vec(new_sentence_index, diction))\n",
    "    labels.append(1) # 1代表负标签\n",
    "    sentences.append(sentence)\n",
    "\n",
    "indices = np.random.permutation(len(dataset))\n",
    "dataset = [dataset[i] for i in indices]\n",
    "labels = [labels[i] for i in indices]\n",
    "sentences = [sentences[i] for i in indices]\n",
    "print(dataset[0][30:128])\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "771ba6aa-867d-455a-a639-84eed4fa318b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1303\n"
     ]
    }
   ],
   "source": [
    "test_size = len(dataset) // 10\n",
    "train_data = dataset[2 * test_size :]\n",
    "train_label = labels[2 * test_size :]\n",
    "\n",
    "validate_data = dataset[: test_size]\n",
    "validate_label = labels[: test_size]\n",
    "\n",
    "test_data = dataset[test_size: 2 * test_size]\n",
    "test_label = labels[test_size: 2 * test_size]\n",
    "\n",
    "print(len(validate_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
