{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e64b0849-4958-4398-9e65-8debb2e74294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演示各种loss函数\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a32e96e3-5755-46cd-b1aa-4bf5c200fb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "手动计算softmax:\n",
      " tensor([[0.2138, 0.2363, 0.2612, 0.2887],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500]])\n",
      "函数计算softmax:\n",
      " tensor([[0.2138, 0.2363, 0.2612, 0.2887],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500]])\n"
     ]
    }
   ],
   "source": [
    "# softmax() function\n",
    "preds = torch.tensor([[0.1, 0.2, 0.3, 0.4], [0.1, 0.1, 0.1, 0.1]])\n",
    "\n",
    "exp = torch.exp(preds)\n",
    "sum_ = torch.sum(exp, dim=1).reshape(-1, 1)\n",
    "softmax = exp / sum_\n",
    "print('手动计算softmax:\\n', softmax)\n",
    "\n",
    "softmax_ = F.softmax(preds, dim=1) # dim=1按行计算\n",
    "print('函数计算softmax:\\n', softmax_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2bf26445-fbdb-4af4-8130-129c6e725af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "手动计算log_softmax:\n",
      " tensor([[-1.5425, -1.4425, -1.3425, -1.2425],\n",
      "        [-1.3863, -1.3863, -1.3863, -1.3863]])\n",
      "函数计算log_softmax:\n",
      " tensor([[-1.5425, -1.4425, -1.3425, -1.2425],\n",
      "        [-1.3863, -1.3863, -1.3863, -1.3863]])\n"
     ]
    }
   ],
   "source": [
    "# log_softmax function\n",
    "preds = torch.tensor([[0.1, 0.2, 0.3, 0.4], [0.1, 0.1, 0.1, 0.1]])\n",
    "\n",
    "exp = torch.exp(preds)\n",
    "sum_ = torch.sum(exp, dim=1).reshape(-1, 1)\n",
    "softmax = exp / sum_\n",
    "log_softmax = torch.log(softmax) \n",
    "print('手动计算log_softmax:\\n', log_softmax)\n",
    "\n",
    "softmax_ = F.log_softmax(preds, dim=1) # dim=1按行计算\n",
    "print('函数计算log_softmax:\\n', softmax_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5f3b0b8b-3d0c-44c4-bef3-941f588d7e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "手动计算nll loss:  tensor(1.3644)\n",
      "函数计算nll loss:  tensor(1.3644)\n"
     ]
    }
   ],
   "source": [
    "# NLL: Negative Log Likelihood 负对数似然\n",
    "preds = torch.tensor([[0.1, 0.2, 0.3, 0.4], [0.1, 0.1, 0.1, 0.1]])\n",
    "target = torch.tensor([2, 3])\n",
    "\n",
    "exp = torch.exp(preds)\n",
    "sum_ = torch.sum(exp, dim=1).reshape(-1, 1)\n",
    "softmax = exp / sum_\n",
    "log_softmax = torch.log(softmax) \n",
    "\n",
    "one_hot = F.one_hot(target).float() \n",
    "nllloss = -torch.sum(one_hot * log_softmax) / target.shape[0]\n",
    "print('手动计算nll loss: ', nllloss)\n",
    "\n",
    "Log_Softmax = F.log_softmax(preds, dim=1)  \n",
    "Nllloss = F.nll_loss(Log_Softmax, target)  \n",
    "print('函数计算nll loss: ', Nllloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6596d995-4482-47a5-a74f-6fe4f0bb8995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]one_hot编码target:\n",
      " tensor([[0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n",
      "[2]对网络预测preds求指数:\n",
      " tensor([[1.1052, 1.2214, 1.3499, 1.4918],\n",
      "        [1.1052, 1.1052, 1.1052, 1.1052]])\n",
      "[3]softmax操作:\n",
      " tensor([[0.2138, 0.2363, 0.2612, 0.2887],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500]])\n",
      "[4]softmax后取对数:\n",
      " tensor([[-1.5425, -1.4425, -1.3425, -1.2425],\n",
      "        [-1.3863, -1.3863, -1.3863, -1.3863]])\n",
      "[5]手动使用nllloss计算交叉熵: tensor(1.3644)\n",
      "----------------------------------------------\n",
      "函数使用Nll loss计算交叉熵: tensor(1.3644)\n",
      "函数交叉熵cross_entropy: tensor(1.3644)\n"
     ]
    }
   ],
   "source": [
    "# Cross Entropy Loss，就是NLL Loss\n",
    "preds = torch.tensor([[0.1, 0.2, 0.3, 0.4], [0.1, 0.1, 0.1, 0.1]])\n",
    "target = torch.tensor([2, 3])\n",
    "\n",
    "one_hot = F.one_hot(target).float() # 对标签作 one_hot 编码\n",
    "print('[1]one_hot编码target:\\n', one_hot)\n",
    "exp = torch.exp(preds)\n",
    "print('[2]对网络预测preds求指数:\\n', exp)\n",
    "sum_ = torch.sum(exp, dim=1).reshape(-1, 1)  # 按行求和\n",
    "softmax = exp / sum_  # 计算 softmax()\n",
    "print('[3]softmax操作:\\n', softmax)\n",
    "log_softmax = torch.log(softmax) # 计算 log_softmax()\n",
    "print('[4]softmax后取对数:\\n', log_softmax)\n",
    "nllloss = -torch.sum(one_hot * log_softmax) / target.shape[0]  # 标签乘以激活后的数据，求平均值，取反\n",
    "print(\"[5]手动使用nllloss计算交叉熵:\", nllloss)\n",
    "\n",
    "print('----------------------------------------------')\n",
    "# 调用 NLLLoss() 函数计算\n",
    "Log_Softmax = F.log_softmax(preds, dim=1)  # log_softmax() 激活\n",
    "Nllloss = F.nll_loss(Log_Softmax, target)  # 无需对标签作 one_hot 编码\n",
    "print(\"函数使用Nll loss计算交叉熵:\", Nllloss)\n",
    "# 直接使用交叉熵损失函数 CrossEntropy_Loss()\n",
    "cross_entropy = F.cross_entropy(preds, target)  # 无需对标签作 one_hot 编码\n",
    "print('函数交叉熵cross_entropy:', cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dc44e242-a3fa-4a35-9dbb-b69dcfc9fbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5425, -1.4425, -1.3425, -1.2425],\n",
      "        [-1.3863, -1.3863, -1.3863, -1.3863]])\n",
      "Label smoothing损失: tensor(1.3669)\n"
     ]
    }
   ],
   "source": [
    "# Label Smoothing：在交叉熵损失CrossEntropy_Loss中，非标签对应位置的预测信息是没有被使用的，\n",
    "# 而Label Smoothing使用了这种信息\n",
    "def linear_combination(x, y, epsilon):\n",
    "    return epsilon * x + (1 - epsilon) * y\n",
    "\n",
    "def reduce_loss(loss, reduction='mean'):\n",
    "    return loss.mean() if reduction == 'mean' else loss.sum() if reduction == 'sum' else loss\n",
    "\n",
    "class LabelSmoothing_CrossEntropy(nn.Module):\n",
    "    def __init__(self, epsilon: float = 0.1, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, preds, target):\n",
    "        n = preds.size()[-1]    \n",
    "        log_preds = F.log_softmax(preds, dim=-1)    \n",
    "        print(log_preds)\n",
    "        # 每一个类别的平均, 即考虑了除指定target之外的其他类别\n",
    "        # -(-1.5425 + -1.4425 + -1.3425 + -1.2425)/4\n",
    "        loss = reduce_loss(-log_preds.sum(dim=-1), self.reduction) # dim: -1 代表按照最里面的维度，也就是4个列（四个类别）相加\n",
    "        nll = F.nll_loss(log_preds, target, reduction=self.reduction)   \n",
    "        return linear_combination(loss / n, nll, self.epsilon)  # n 样本数\n",
    "\n",
    "preds = torch.tensor([[0.1, 0.2, 0.3, 0.4], [0.1, 0.1, 0.1, 0.1]])\n",
    "target = torch.tensor([2, 3])\n",
    "\n",
    "ls = LabelSmoothing_CrossEntropy()\n",
    "lsloss = ls(preds, target)\n",
    "print('Label smoothing损失:', lsloss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
